{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAC_Hover_initial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQkCOYauom9O+xZDrGenYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjanavasudevan/Thesis/blob/main/Quadrotor_simulation/controllers/DeepRL/SAC_Hover_initial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ldkV2b8EqnLd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from torch.autograd import grad\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "#from tensorboardX import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hover_state_space import hover_state_space\n",
        "env = hover_state_space()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8AZKhbJqugy",
        "outputId": "a924a986-f41e-4f85-ac46-c85c71b8d8be"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An_7n39HyFW9",
        "outputId": "47518d48-66e8-4a17-9454-4011ae99482c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x51wQ9j0-3Bu",
        "outputId": "bd130319-2768-417b-864a-9dc7eba89966"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "    def action(self, action):\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
        "        action = np.clip(action, low, high)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def _reverse_action(self, action):\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        action = 2 * (action - low) / (high - low) - 1\n",
        "        action = np.clip(action, low, high)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "env = NormalizedActions(env)"
      ],
      "metadata": {
        "id": "bIrlvPh-qzrK"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tau = 0.005\n",
        "target_update_interval = 1\n",
        "gradient_steps = 1\n",
        "lr = 3e-4\n",
        "gamma = 0.99\n",
        "capacity = 10000\n",
        "iteration = 1000\n",
        "batch_size = 64\n",
        "num_hidden_layers = 2\n",
        "num_hidden_units_per_layer = 128\n",
        "sample_frequency = 256\n",
        "activation = 'Relu'\n",
        "log_interval = 2000\n",
        "\n",
        "max_action = float(env.action_space.high[0])\n",
        "min_Val = torch.tensor(1e-7).float()\n",
        "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_', 'd'])"
      ],
      "metadata": {
        "id": "8x2VZFJJsfh_"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, min_log_std=-20, max_log_std=2):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.mu_head = nn.Linear(256, 4)\n",
        "        self.log_std_head = nn.Linear(256, 4)\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.min_log_std = min_log_std\n",
        "        self.max_log_std = max_log_std\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mu = self.mu_head(x)\n",
        "        log_std_head = F.relu(self.log_std_head(x))\n",
        "        log_std_head = torch.clamp(log_std_head, self.min_log_std, self.max_log_std)\n",
        "        return mu, log_std_head"
      ],
      "metadata": {
        "id": "NHpwqB-_tk1o"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LyD9Mu13tm8f"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Q(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Q, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        #print(s.shape, a.shape)\n",
        "        #s = s.reshape(-1, state_dim)\n",
        "        #print(s.shape)\n",
        "        #print(a)\n",
        "        a = a.reshape(-1, action_dim)\n",
        "        #print(a.shape)\n",
        "        x = torch.cat([s, a], -1) # combination s and a\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cL99jnwEtowH"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAC():\n",
        "    def __init__(self):\n",
        "        super(SAC, self).__init__()\n",
        "\n",
        "        self.policy_net = Actor(state_dim).to(device)\n",
        "        self.value_net = Critic(state_dim).to(device)\n",
        "        self.Q_net = Q(state_dim, action_dim).to(device)\n",
        "        self.Target_value_net = Critic(state_dim).to(device)\n",
        "\n",
        "        self.replay_buffer = [Transition] * capacity\n",
        "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
        "        self.Q_optimizer = optim.Adam(self.Q_net.parameters(), lr=lr)\n",
        "        self.num_transition = 0 # pointer of replay buffer\n",
        "        self.num_training = 1\n",
        "        #self.writer = SummaryWriter('./exp-SAC')\n",
        "\n",
        "        self.value_criterion = nn.MSELoss()\n",
        "        self.Q_criterion = nn.MSELoss()\n",
        "\n",
        "        for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        os.makedirs('./SAC_model/', exist_ok=True)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        mu, log_sigma = self.policy_net(state)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        dist = Normal(mu, sigma)\n",
        "        z = dist.sample()\n",
        "        action = torch.tanh(z).detach().cpu().numpy()\n",
        "        return action # return a scalar, float32\n",
        "\n",
        "    def store(self, s, a, r, s_, d):\n",
        "        index = self.num_transition % capacity\n",
        "        #print(\"Action shape: \", a.shape)\n",
        "        transition = Transition(s, a, r, s_, d)\n",
        "        self.replay_buffer[index] = transition\n",
        "        self.num_transition += 1\n",
        "\n",
        "    def get_action_log_prob(self, state):\n",
        "\n",
        "        batch_mu, batch_log_sigma = self.policy_net(state)\n",
        "        batch_sigma = torch.exp(batch_log_sigma)\n",
        "        dist = Normal(batch_mu, batch_sigma)\n",
        "        z = dist.sample()\n",
        "        action = torch.tanh(z)\n",
        "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + min_Val)\n",
        "        return action, log_prob, z, batch_mu, batch_log_sigma\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        if self.num_training % 500 == 0:\n",
        "            print(\"Training ... {} \".format(self.num_training))\n",
        "        s = torch.tensor(np.array([t.s for t in self.replay_buffer])).float().to(device)\n",
        "        a = torch.tensor(np.array([t.a for t in self.replay_buffer])).to(device)\n",
        "        r = torch.tensor(np.array([t.r for t in self.replay_buffer])).to(device)\n",
        "        s_ = torch.tensor(np.array([t.s_ for t in self.replay_buffer])).float().to(device)\n",
        "        d = torch.tensor(np.array([t.d for t in self.replay_buffer])).float().to(device)\n",
        "\n",
        "        for _ in range(gradient_steps):\n",
        "            #for index in BatchSampler(SubsetRandomSampler(range(args.capacity)), args.batch_size, False):\n",
        "            index = np.random.choice(range(capacity), batch_size, replace=False)\n",
        "            #print(s.shape)\n",
        "            bn_s = s[index]\n",
        "            #print(a.shape)\n",
        "            bn_a = a[index].reshape(-1, 1)\n",
        "            bn_r = r[index].reshape(-1, 1)\n",
        "            bn_s_ = s_[index]\n",
        "            bn_d = d[index].reshape(-1, 1)\n",
        "\n",
        "\n",
        "            target_value = self.Target_value_net(bn_s_)\n",
        "            next_q_value = bn_r + (1 - bn_d) * gamma * target_value\n",
        "\n",
        "            excepted_value = self.value_net(bn_s)\n",
        "            excepted_Q = self.Q_net(bn_s, bn_a)\n",
        "\n",
        "            sample_action, log_prob, z, batch_mu, batch_log_sigma = self.get_action_log_prob(bn_s)\n",
        "            excepted_new_Q = self.Q_net(bn_s, sample_action)\n",
        "            next_value = excepted_new_Q - log_prob\n",
        "\n",
        "            # !!!Note that the actions are sampled according to the current policy,\n",
        "            # instead of replay buffer. (From original paper)\n",
        "\n",
        "            V_loss = self.value_criterion(excepted_value, next_value.detach())  # J_V\n",
        "            V_loss = V_loss.mean()\n",
        "            v_loss_copy = V_loss.clone().detach().cpu().numpy()\n",
        "\n",
        "            # Single Q_net this is different from original paper!!!\n",
        "            Q_loss = self.Q_criterion(excepted_Q.float(), next_q_value.detach().float()) # J_Q\n",
        "            Q_loss = Q_loss.mean().float()\n",
        "            q_loss_copy = Q_loss.clone().detach().cpu().numpy()\n",
        "\n",
        "            log_policy_target = excepted_new_Q - excepted_value\n",
        "\n",
        "            pi_loss = log_prob * (log_prob- log_policy_target).detach()\n",
        "            pi_loss = pi_loss.mean()\n",
        "            pi_loss_copy = pi_loss.clone().detach().cpu().numpy()\n",
        "\n",
        "            #self.writer.add_scalar('Loss/V_loss', V_loss, global_step=self.num_training)\n",
        "            #self.writer.add_scalar('Loss/Q_loss', Q_loss, global_step=self.num_training)\n",
        "            #self.writer.add_scalar('Loss/pi_loss', pi_loss, global_step=self.num_training)\n",
        "            # mini batch gradient descent\n",
        "            self.value_optimizer.zero_grad()\n",
        "            V_loss.backward(retain_graph=True)\n",
        "            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "            self.Q_optimizer.zero_grad()\n",
        "            Q_loss.backward(retain_graph = True)\n",
        "            nn.utils.clip_grad_norm_(self.Q_net.parameters(), 0.5)\n",
        "            self.Q_optimizer.step()\n",
        "\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            pi_loss.backward(retain_graph = True)\n",
        "            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n",
        "            self.policy_optimizer.step()\n",
        "\n",
        "            # soft update\n",
        "            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n",
        "                target_param.data.copy_(target_param * (1 - tau) + param * tau)\n",
        "\n",
        "            self.num_training += 1\n",
        "            return v_loss_copy, q_loss_copy, pi_loss_copy\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.policy_net.state_dict(), './SAC_model/policy_net.pth')\n",
        "        torch.save(self.value_net.state_dict(), './SAC_model/value_net.pth')\n",
        "        torch.save(self.Q_net.state_dict(), './SAC_model/Q_net.pth')\n",
        "        print(\"====================================\")\n",
        "        print(\"Model has been saved...\")\n",
        "        print(\"====================================\")\n",
        "\n",
        "    def load(self):\n",
        "        torch.load(self.policy_net.state_dict(), './SAC_model/policy_net.pth')\n",
        "        torch.load(self.value_net.state_dict(), './SAC_model/value_net.pth')\n",
        "        torch.load(self.Q_net.state_dict(), './SAC_model/Q_net.pth')\n",
        "        print()"
      ],
      "metadata": {
        "id": "9cL4tyE7t5f-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent = SAC()\n",
        "#if args.load: agent.load()\n",
        "#if args.render: env.render()\n",
        "print(\"====================================\")\n",
        "print(\"Collection Experience...\")\n",
        "print(\"====================================\")\n",
        "\n",
        "#ep_r = 0\n",
        "rewards = []\n",
        "q_loss = []\n",
        "v_loss = []\n",
        "pi_loss = []\n",
        "for i in range(iteration):\n",
        "  ep_loss = dict(q_loss = [], v_loss = [], pi_loss = [])\n",
        "  ep_r = 0\n",
        "  state = env.reset()\n",
        "  for t in range(200):\n",
        "    action = agent.select_action(state)\n",
        "    #print(\"Action shape\", action.shape)\n",
        "    next_state, reward, done = env.step(action)\n",
        "    ep_r += reward\n",
        "    #if args.render: env.render()\n",
        "    agent.store(state, action, reward, next_state, done)\n",
        "\n",
        "    if agent.num_transition >= capacity:\n",
        "      q_l, v_l, pi_l = agent.update()\n",
        "      ep_loss['q_loss'].append(float(q_l))\n",
        "      ep_loss['v_loss'].append(float(v_l))\n",
        "      ep_loss['pi_loss'].append(float(pi_l))\n",
        "\n",
        "    state = next_state\n",
        "    if done or t == 199:\n",
        "      if i % 20 == 0:\n",
        "        print(\"Ep_i {}, the ep_r is {}, the t is {}\".format(i, ep_r, t))\n",
        "        break\n",
        "  if i % log_interval == 0:\n",
        "    agent.save()\n",
        "  #agent.writer.add_scalar('ep_r', ep_r, global_step=i)\n",
        "  #agent.writer.add_scalar('Episode_q_loss', np.mean(ep_loss['q_loss']), global_step=i)\n",
        "  #agent.writer.add_scalar('Episode_v_loss', np.mean(ep_loss['v_loss']), global_step=i)\n",
        "  #agent.writer.add_scalar('Episode_pi_loss', np.mean(ep_loss['pi_loss']), global_step=i)\n",
        "        \n",
        "  rewards.append(ep_r)\n",
        "  q_loss.append(np.mean(ep_loss['q_loss']))\n",
        "  v_loss.append(np.mean(ep_loss['v_loss']))\n",
        "  pi_loss.append(np.mean(ep_loss['pi_loss']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-kkgCezuDPG",
        "outputId": "3eeb6a52-b287-4dcf-e216-d69f41e096d7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================\n",
            "Collection Experience...\n",
            "====================================\n",
            "Ep_i 0, the ep_r is -11624.684025995652, the t is 199\n",
            "====================================\n",
            "Model has been saved...\n",
            "====================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep_i 20, the ep_r is -6553.809459206724, the t is 199\n",
            "Ep_i 40, the ep_r is -9660.900106177303, the t is 199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([64, 4])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ... 500 \n",
            "Training ... 1000 \n",
            "Training ... 1500 \n",
            "Training ... 2000 \n",
            "Ep_i 60, the ep_r is -12843.619592600611, the t is 199\n",
            "Training ... 2500 \n",
            "Training ... 3000 \n",
            "Training ... 3500 \n",
            "Training ... 4000 \n",
            "Training ... 4500 \n",
            "Training ... 5000 \n",
            "Training ... 5500 \n",
            "Training ... 6000 \n",
            "Ep_i 80, the ep_r is -1904.5465126778033, the t is 199\n",
            "Training ... 6500 \n",
            "Training ... 7000 \n",
            "Training ... 7500 \n",
            "Training ... 8000 \n",
            "Training ... 8500 \n",
            "Training ... 9000 \n",
            "Training ... 9500 \n",
            "Training ... 10000 \n",
            "Ep_i 100, the ep_r is -2875.924235787736, the t is 199\n",
            "Training ... 10500 \n",
            "Training ... 11000 \n",
            "Training ... 11500 \n",
            "Training ... 12000 \n",
            "Training ... 12500 \n",
            "Training ... 13000 \n",
            "Training ... 13500 \n",
            "Training ... 14000 \n",
            "Ep_i 120, the ep_r is -947.0357072903449, the t is 199\n",
            "Training ... 14500 \n",
            "Training ... 15000 \n",
            "Training ... 15500 \n",
            "Training ... 16000 \n",
            "Training ... 16500 \n",
            "Training ... 17000 \n",
            "Training ... 17500 \n",
            "Training ... 18000 \n",
            "Ep_i 140, the ep_r is -825.8961688523301, the t is 199\n",
            "Training ... 18500 \n",
            "Training ... 19000 \n",
            "Training ... 19500 \n",
            "Training ... 20000 \n",
            "Training ... 20500 \n",
            "Training ... 21000 \n",
            "Training ... 21500 \n",
            "Training ... 22000 \n",
            "Ep_i 160, the ep_r is -1552.2838040699796, the t is 199\n",
            "Training ... 22500 \n",
            "Training ... 23000 \n",
            "Training ... 23500 \n",
            "Training ... 24000 \n",
            "Training ... 24500 \n",
            "Training ... 25000 \n",
            "Training ... 25500 \n",
            "Training ... 26000 \n",
            "Ep_i 180, the ep_r is -1071.874585667462, the t is 199\n",
            "Training ... 26500 \n",
            "Training ... 27000 \n",
            "Training ... 27500 \n",
            "Training ... 28000 \n",
            "Training ... 28500 \n",
            "Training ... 29000 \n",
            "Training ... 29500 \n",
            "Training ... 30000 \n",
            "Ep_i 200, the ep_r is -999.5307982819196, the t is 199\n",
            "Training ... 30500 \n",
            "Training ... 31000 \n",
            "Training ... 31500 \n",
            "Training ... 32000 \n",
            "Training ... 32500 \n",
            "Training ... 33000 \n",
            "Training ... 33500 \n",
            "Training ... 34000 \n",
            "Ep_i 220, the ep_r is -876.0552405218311, the t is 199\n",
            "Training ... 34500 \n",
            "Training ... 35000 \n",
            "Training ... 35500 \n",
            "Training ... 36000 \n",
            "Training ... 36500 \n",
            "Training ... 37000 \n",
            "Training ... 37500 \n",
            "Training ... 38000 \n",
            "Ep_i 240, the ep_r is -940.3133527652946, the t is 199\n",
            "Training ... 38500 \n",
            "Training ... 39000 \n",
            "Training ... 39500 \n",
            "Training ... 40000 \n",
            "Training ... 40500 \n",
            "Training ... 41000 \n",
            "Training ... 41500 \n",
            "Training ... 42000 \n",
            "Ep_i 260, the ep_r is -1950.69328204626, the t is 199\n",
            "Training ... 42500 \n",
            "Training ... 43000 \n",
            "Training ... 43500 \n",
            "Training ... 44000 \n",
            "Training ... 44500 \n",
            "Training ... 45000 \n",
            "Training ... 45500 \n",
            "Training ... 46000 \n",
            "Ep_i 280, the ep_r is -984.9080468920688, the t is 199\n",
            "Training ... 46500 \n",
            "Training ... 47000 \n",
            "Training ... 47500 \n",
            "Training ... 48000 \n",
            "Training ... 48500 \n",
            "Training ... 49000 \n",
            "Training ... 49500 \n",
            "Training ... 50000 \n",
            "Ep_i 300, the ep_r is -2097.8201926449333, the t is 199\n",
            "Training ... 50500 \n",
            "Training ... 51000 \n",
            "Training ... 51500 \n",
            "Training ... 52000 \n",
            "Training ... 52500 \n",
            "Training ... 53000 \n",
            "Training ... 53500 \n",
            "Training ... 54000 \n",
            "Ep_i 320, the ep_r is -803.1305624092363, the t is 199\n",
            "Training ... 54500 \n",
            "Training ... 55000 \n",
            "Training ... 55500 \n",
            "Training ... 56000 \n",
            "Training ... 56500 \n",
            "Training ... 57000 \n",
            "Training ... 57500 \n",
            "Training ... 58000 \n",
            "Ep_i 340, the ep_r is -2739.271949782731, the t is 199\n",
            "Training ... 58500 \n",
            "Training ... 59000 \n",
            "Training ... 59500 \n",
            "Training ... 60000 \n",
            "Training ... 60500 \n",
            "Training ... 61000 \n",
            "Training ... 61500 \n",
            "Training ... 62000 \n",
            "Ep_i 360, the ep_r is -2920.3493453398705, the t is 199\n",
            "Training ... 62500 \n",
            "Training ... 63000 \n",
            "Training ... 63500 \n",
            "Training ... 64000 \n",
            "Training ... 64500 \n",
            "Training ... 65000 \n",
            "Training ... 65500 \n",
            "Training ... 66000 \n",
            "Ep_i 380, the ep_r is -2586.376723482177, the t is 199\n",
            "Training ... 66500 \n",
            "Training ... 67000 \n",
            "Training ... 67500 \n",
            "Training ... 68000 \n",
            "Training ... 68500 \n",
            "Training ... 69000 \n",
            "Training ... 69500 \n",
            "Training ... 70000 \n",
            "Ep_i 400, the ep_r is -6359.0042243338585, the t is 199\n",
            "Training ... 70500 \n",
            "Training ... 71000 \n",
            "Training ... 71500 \n",
            "Training ... 72000 \n",
            "Training ... 72500 \n",
            "Training ... 73000 \n",
            "Training ... 73500 \n",
            "Training ... 74000 \n",
            "Ep_i 420, the ep_r is -806.3350724268197, the t is 199\n",
            "Training ... 74500 \n",
            "Training ... 75000 \n",
            "Training ... 75500 \n",
            "Training ... 76000 \n",
            "Training ... 76500 \n",
            "Training ... 77000 \n",
            "Training ... 77500 \n",
            "Training ... 78000 \n",
            "Ep_i 440, the ep_r is -4390.2700415590325, the t is 199\n",
            "Training ... 78500 \n",
            "Training ... 79000 \n",
            "Training ... 79500 \n",
            "Training ... 80000 \n",
            "Training ... 80500 \n",
            "Training ... 81000 \n",
            "Training ... 81500 \n",
            "Training ... 82000 \n",
            "Ep_i 460, the ep_r is -2429.4981633555662, the t is 199\n",
            "Training ... 82500 \n",
            "Training ... 83000 \n",
            "Training ... 83500 \n",
            "Training ... 84000 \n",
            "Training ... 84500 \n",
            "Training ... 85000 \n",
            "Training ... 85500 \n",
            "Training ... 86000 \n",
            "Ep_i 480, the ep_r is -1149.1127829334023, the t is 199\n",
            "Training ... 86500 \n",
            "Training ... 87000 \n",
            "Training ... 87500 \n",
            "Training ... 88000 \n",
            "Training ... 88500 \n",
            "Training ... 89000 \n",
            "Training ... 89500 \n",
            "Training ... 90000 \n",
            "Ep_i 500, the ep_r is -4385.103254459009, the t is 199\n",
            "Training ... 90500 \n",
            "Training ... 91000 \n",
            "Training ... 91500 \n",
            "Training ... 92000 \n",
            "Training ... 92500 \n",
            "Training ... 93000 \n",
            "Training ... 93500 \n",
            "Training ... 94000 \n",
            "Ep_i 520, the ep_r is -1667.3236360491528, the t is 199\n",
            "Training ... 94500 \n",
            "Training ... 95000 \n",
            "Training ... 95500 \n",
            "Training ... 96000 \n",
            "Training ... 96500 \n",
            "Training ... 97000 \n",
            "Training ... 97500 \n",
            "Training ... 98000 \n",
            "Ep_i 540, the ep_r is -1778.8220487487063, the t is 199\n",
            "Training ... 98500 \n",
            "Training ... 99000 \n",
            "Training ... 99500 \n",
            "Training ... 100000 \n",
            "Training ... 100500 \n",
            "Training ... 101000 \n",
            "Training ... 101500 \n",
            "Training ... 102000 \n",
            "Ep_i 560, the ep_r is -1178.7461863219064, the t is 199\n",
            "Training ... 102500 \n",
            "Training ... 103000 \n",
            "Training ... 103500 \n",
            "Training ... 104000 \n",
            "Training ... 104500 \n",
            "Training ... 105000 \n",
            "Training ... 105500 \n",
            "Training ... 106000 \n",
            "Ep_i 580, the ep_r is -6173.142297598164, the t is 199\n",
            "Training ... 106500 \n",
            "Training ... 107000 \n",
            "Training ... 107500 \n",
            "Training ... 108000 \n",
            "Training ... 108500 \n",
            "Training ... 109000 \n",
            "Training ... 109500 \n",
            "Training ... 110000 \n",
            "Ep_i 600, the ep_r is -1947.8979533335885, the t is 199\n",
            "Training ... 110500 \n",
            "Training ... 111000 \n",
            "Training ... 111500 \n",
            "Training ... 112000 \n",
            "Training ... 112500 \n",
            "Training ... 113000 \n",
            "Training ... 113500 \n",
            "Training ... 114000 \n",
            "Ep_i 620, the ep_r is -1400.63424188923, the t is 199\n",
            "Training ... 114500 \n",
            "Training ... 115000 \n",
            "Training ... 115500 \n",
            "Training ... 116000 \n",
            "Training ... 116500 \n",
            "Training ... 117000 \n",
            "Training ... 117500 \n",
            "Training ... 118000 \n",
            "Ep_i 640, the ep_r is -1716.716780503587, the t is 199\n",
            "Training ... 118500 \n",
            "Training ... 119000 \n",
            "Training ... 119500 \n",
            "Training ... 120000 \n",
            "Training ... 120500 \n",
            "Training ... 121000 \n",
            "Training ... 121500 \n",
            "Training ... 122000 \n",
            "Ep_i 660, the ep_r is -883.6782488979135, the t is 199\n",
            "Training ... 122500 \n",
            "Training ... 123000 \n",
            "Training ... 123500 \n",
            "Training ... 124000 \n",
            "Training ... 124500 \n",
            "Training ... 125000 \n",
            "Training ... 125500 \n",
            "Training ... 126000 \n",
            "Ep_i 680, the ep_r is -801.3543843140457, the t is 199\n",
            "Training ... 126500 \n",
            "Training ... 127000 \n",
            "Training ... 127500 \n",
            "Training ... 128000 \n",
            "Training ... 128500 \n",
            "Training ... 129000 \n",
            "Training ... 129500 \n",
            "Training ... 130000 \n",
            "Ep_i 700, the ep_r is -1058.7206144804836, the t is 199\n",
            "Training ... 130500 \n",
            "Training ... 131000 \n",
            "Training ... 131500 \n",
            "Training ... 132000 \n",
            "Training ... 132500 \n",
            "Training ... 133000 \n",
            "Training ... 133500 \n",
            "Training ... 134000 \n",
            "Ep_i 720, the ep_r is -1006.7843078669054, the t is 199\n",
            "Training ... 134500 \n",
            "Training ... 135000 \n",
            "Training ... 135500 \n",
            "Training ... 136000 \n",
            "Training ... 136500 \n",
            "Training ... 137000 \n",
            "Training ... 137500 \n",
            "Training ... 138000 \n",
            "Ep_i 740, the ep_r is -2611.862593613648, the t is 199\n",
            "Training ... 138500 \n",
            "Training ... 139000 \n",
            "Training ... 139500 \n",
            "Training ... 140000 \n",
            "Training ... 140500 \n",
            "Training ... 141000 \n",
            "Training ... 141500 \n",
            "Training ... 142000 \n",
            "Ep_i 760, the ep_r is -4636.460361456643, the t is 199\n",
            "Training ... 142500 \n",
            "Training ... 143000 \n",
            "Training ... 143500 \n",
            "Training ... 144000 \n",
            "Training ... 144500 \n",
            "Training ... 145000 \n",
            "Training ... 145500 \n",
            "Training ... 146000 \n",
            "Ep_i 780, the ep_r is -4963.136434901578, the t is 199\n",
            "Training ... 146500 \n",
            "Training ... 147000 \n",
            "Training ... 147500 \n",
            "Training ... 148000 \n",
            "Training ... 148500 \n",
            "Training ... 149000 \n",
            "Training ... 149500 \n",
            "Training ... 150000 \n",
            "Ep_i 800, the ep_r is -1531.577306555701, the t is 199\n",
            "Training ... 150500 \n",
            "Training ... 151000 \n",
            "Training ... 151500 \n",
            "Training ... 152000 \n",
            "Training ... 152500 \n",
            "Training ... 153000 \n",
            "Training ... 153500 \n",
            "Training ... 154000 \n",
            "Ep_i 820, the ep_r is -2798.4976351281393, the t is 199\n",
            "Training ... 154500 \n",
            "Training ... 155000 \n",
            "Training ... 155500 \n",
            "Training ... 156000 \n",
            "Training ... 156500 \n",
            "Training ... 157000 \n",
            "Training ... 157500 \n",
            "Training ... 158000 \n",
            "Ep_i 840, the ep_r is -8699.14038222213, the t is 199\n",
            "Training ... 158500 \n",
            "Training ... 159000 \n",
            "Training ... 159500 \n",
            "Training ... 160000 \n",
            "Training ... 160500 \n",
            "Training ... 161000 \n",
            "Training ... 161500 \n",
            "Training ... 162000 \n",
            "Ep_i 860, the ep_r is -6015.316244913295, the t is 199\n",
            "Training ... 162500 \n",
            "Training ... 163000 \n",
            "Training ... 163500 \n",
            "Training ... 164000 \n",
            "Training ... 164500 \n",
            "Training ... 165000 \n",
            "Training ... 165500 \n",
            "Training ... 166000 \n",
            "Ep_i 880, the ep_r is -3875.4034416355858, the t is 199\n",
            "Training ... 166500 \n",
            "Training ... 167000 \n",
            "Training ... 167500 \n",
            "Training ... 168000 \n",
            "Training ... 168500 \n",
            "Training ... 169000 \n",
            "Training ... 169500 \n",
            "Training ... 170000 \n",
            "Ep_i 900, the ep_r is -3821.280225045087, the t is 199\n",
            "Training ... 170500 \n",
            "Training ... 171000 \n",
            "Training ... 171500 \n",
            "Training ... 172000 \n",
            "Training ... 172500 \n",
            "Training ... 173000 \n",
            "Training ... 173500 \n",
            "Training ... 174000 \n",
            "Ep_i 920, the ep_r is -6241.011255619944, the t is 199\n",
            "Training ... 174500 \n",
            "Training ... 175000 \n",
            "Training ... 175500 \n",
            "Training ... 176000 \n",
            "Training ... 176500 \n",
            "Training ... 177000 \n",
            "Training ... 177500 \n",
            "Training ... 178000 \n",
            "Ep_i 940, the ep_r is -4737.507002588441, the t is 199\n",
            "Training ... 178500 \n",
            "Training ... 179000 \n",
            "Training ... 179500 \n",
            "Training ... 180000 \n",
            "Training ... 180500 \n",
            "Training ... 181000 \n",
            "Training ... 181500 \n",
            "Training ... 182000 \n",
            "Ep_i 960, the ep_r is -1379.5306478229643, the t is 199\n",
            "Training ... 182500 \n",
            "Training ... 183000 \n",
            "Training ... 183500 \n",
            "Training ... 184000 \n",
            "Training ... 184500 \n",
            "Training ... 185000 \n",
            "Training ... 185500 \n",
            "Training ... 186000 \n",
            "Ep_i 980, the ep_r is -6659.387640017392, the t is 199\n",
            "Training ... 186500 \n",
            "Training ... 187000 \n",
            "Training ... 187500 \n",
            "Training ... 188000 \n",
            "Training ... 188500 \n",
            "Training ... 189000 \n",
            "Training ... 189500 \n",
            "Training ... 190000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([[ 0.8846],\n",
        "        [-0.7883],\n",
        "        [-0.7953],\n",
        "        [-0.1356],\n",
        "        [-0.3132],\n",
        "        [ 0.2543],\n",
        "        [ 0.1001],\n",
        "        [ 0.3229],\n",
        "        [-0.0241],\n",
        "        [ 0.7039],\n",
        "        [-0.0283],\n",
        "        [ 0.3579],\n",
        "        [ 0.3291],\n",
        "        [-0.7184],\n",
        "        [-0.4124],\n",
        "        [ 0.4838],\n",
        "        [-0.2666],\n",
        "        [-0.5899],\n",
        "        [-0.9577],\n",
        "        [-0.5541],\n",
        "        [ 0.9838],\n",
        "        [-0.3028],\n",
        "        [-0.8259],\n",
        "        [-0.8602],\n",
        "        [-0.7234],\n",
        "        [ 0.1965],\n",
        "        [-0.0496],\n",
        "        [-0.1460],\n",
        "        [-0.0028],\n",
        "        [ 0.5260],\n",
        "        [ 0.6380],\n",
        "        [-0.8950],\n",
        "        [ 0.4189],\n",
        "        [-0.6324],\n",
        "        [-0.6256],\n",
        "        [-0.7158],\n",
        "        [-0.9768],\n",
        "        [-0.9586],\n",
        "        [-0.7450],\n",
        "        [-0.9746],\n",
        "        [ 0.3168],\n",
        "        [-0.1032],\n",
        "        [ 0.0153],\n",
        "        [ 0.4876],\n",
        "        [ 0.4393],\n",
        "        [ 0.7463],\n",
        "        [-0.7817],\n",
        "        [-0.9114],\n",
        "        [-0.0041],\n",
        "        [-0.4704],\n",
        "        [-0.9687],\n",
        "        [ 0.7959],\n",
        "        [-0.7933],\n",
        "        [-0.9439],\n",
        "        [-0.8123],\n",
        "        [ 0.7601],\n",
        "        [-0.9367],\n",
        "        [ 0.0550],\n",
        "        [ 0.6610],\n",
        "        [-0.6241],\n",
        "        [ 0.2836],\n",
        "        [-0.9984],\n",
        "        [ 0.3965],\n",
        "        [-0.3901]])\n",
        "a.shape"
      ],
      "metadata": {
        "id": "jkLnewCBAlUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c833e54-4e67-4886-d4d3-9a7aa2b49201"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "h_XXvywSMRtb",
        "outputId": "4e5a2b34-d876-497e-8461-d51235059ac5"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1568733390>]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD6CAYAAACyNXAiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgU1bXAf6dnY98RkEVQQAIoLqjgvqDiElHjghqj0YQY9cXE5CX4jFk0JiZGzaaJJmrUGI1rxBUFcRcVxA0QGQEFRBZZZRlmpu/7o6u6q6urqqu6q2d6hvP7vvmm+tatW7e2e+4599xzxRiDoiiKouQj0dwVUBRFUVoGKjAURVGUUKjAUBRFUUKhAkNRFEUJhQoMRVEUJRQqMBRFUZRQtCiBISLjRWSBiNSKyOTmro+iKMqOhLSUeRgiUgF8BBwNLAPeAs4yxszzyt+jRw8zcODApqugoihKK2D27NlrjDE9vfZVNnVlimB/oNYYswhARO4HJgCeAmPgwIHMmjWrCaunKIrS8hGRT/z2tSSTVF9gqeP3MitNURRFaQJaksDIi4hMEpFZIjJr9erVzV0dRVGUVkVLEhjLgf6O3/2stDTGmNuMMaONMaN79vQ0wSmKoigF0pIExlvAEBEZJCLVwERgSjPXSVEUZYehxQx6G2MaRORSYCpQAdxhjJnbzNVSFEXZYWgxAgPAGPMU8FRz10NRFGVHpCWZpBRFUZRmRAVGK2Plxm08O/fz5q6GoiitEBUYrYxTbn6VSffMptAZ/A2NSS67fw7vLl0fS31erV3DRys3RT5u07Z6ttU3pn9vq2+kvjEZqYyGxiRrvqzLSvvw840M/9kzzF+x0fMYYwwbttb77rt+6ocstK6noTHpeZ/Xb9nOL6bMZe3m7Wyua8AYw7J1W9L7P1i+gTNvfZ1t9Y2s2rQt69jVm+r48/SFWeXa2x+v/pL3lqWeSzJpuOaJeXywfEM638sLV3PXa0t4cNbSrDJnf7KO5+atzKnnlu0NbG/I3NOVG7exdO2WnHxR+Gz9VpLJwt69lRu38cjby/iyriFv3rWbt/On6QuzztWYNNz43Eds2FrPSx+tZv2W7ek6LVr9pW9ZC1duonZVZv/NM2q5781PA8+/dO0WXv/4CwBeWLCKJWs25+RZsmYzqzamnu+1T87jxmcX8OzczzHG8Pan61i4chPvLVvPUTe8wHvL1nP0jS+y0sr/j5cX8bW/vgbAax+v4TdPz/ety+pNdfzkofeyvpdSoQKjBbJhaz3vLF3P5Iff4zTrpbL5bEPqhWt0fbSv1a7hiN+/EPhSvbdsPe8uW89j73zGz6fMpaExybR5K/l8wzYGTn6S12rXAPDKwjXM+XRdqBf0nH+8wTE3vZQ33+a6hqzy9vjFswy76hlueaEWgGFXPcMZt76edcy2+kbPxqm+MUkyabjikfcZ/atpWYLmXzM/Ycv2Ro7748tMefczjDFZjeQdry5h1C+fTTfwz879nNWbUkJnw9Z6bp7xMUff9BKNScPgK5/muqc/TB/78OxlDJz8JD+fMpd/vraEfa55jgOve557Zn7Cwb+dwSX3vs2jc5bx0/9+wBuL13Lri4vY/9rpPP9hpjG//IF3uOG5j5j88PvcPKOWf838hEFXPMVDs5dx1A0vctJfXgVgztL13P7KYq5+IhPo4Nzb3+TnU+byvw+9l3U/vvbX1/j23blRD4b/bCpDf/o00+evZPGazRzw6+kc8rsZbNqWer/8MMYw+5N1OcJyxoerOPC653nG0nDrfQSqHwf8ejqXP/Au59/xJnUNue/WP19dzMKVmzDGcMm9b3Pjcx8x97OM4P/Hy4v40/SF/PCBd/nGHW/ynXtmA3Dgdc9z5A0vep7zlYVrOPqmlxh3Y2b/9VMXcMUj71PfmKQxabh5Rm36HbC54pH3OevvM1m2bgvn3/kWh//+BYwxLPh8Ew/NTgm9w3//Avv/ejoAf395MX96vpZJ98zm4beXc+otr3H0TS/x3X+9zcerN3PSX15l4aovOeDX07nn9SX86sn5zP5kHQBn//0Nbn1xUfrcG7bU8/upC9Lfy++nLuA/s5ay19XPsuDzTazbvD30PY9Kixr03lF54r3P2LC1nrP3H4CIcOatr/Ph58G99oakobIi8/uqxz5g8ZrNfLp2C0N7dQRSPdb21ZX07tyG5z9cyQX/nMUefTsDMLB7O/724sf8/tmPmLhfavrL7a8s5udT5rLQ0Rtbct0JvnW467Ul6e3l67fSo0M1Nc5Kkerl3vfmUq6xGr55Vx9Lu+rMa/m7ZxZwz+upSAVzPk01YnM/20CHmkoOu/4Fvj5mAL86eY+sMve55jmqKhKstT6c+sYkVRUJvqxrYNHqTE/we/fNYe2Xdfzi8Xk88T8HM7JvZ55+fwUAB/92BvsM6MLbn65nWO+OPPP9Q2lwCKct21O94FtfWsQVx38ldb2vL7Hql2nENmytT/fun3x/BU++v4Kv9OkEwEerUs/w5YVrGLlzZ47/0yvpRuA/Li3hRw++m/XbFnI1ld59vhcWrGJk38706FCTs29bfWNWh+LCu2Zx/6Qx6d9n3DqT+Ss2cs2EEXy6dgtXnjAcSGl9basqeGbu51z67znccPoovrZvv/RxT3+QunePvL2Mi+99G4CrJ4zgG2MHpu7FlnqqKoVVG+vo1qGaG6Yu4PvjhtK1fXVW/WZ9so6DfzuDt64cl1XnXzyeekeOHdGL1xelevcimeN+YwnvafNT9/uTL7K1pYGTn2Ta5Yexa4/2LFu3leufXcDj737mef8Ahlz5NHsP6MKcT9dz/dQFPHLxgewzoCsAr1idpxuf/Sidf/r8Vdz8Qi1zPl2f87ycOIXP8vVbc/Zf9VjG+XPjtoy2++kXW1iwchMPzlrKs/NW8pcZtTx+6cFsqkvl2Vaf5Ng/pDpnQd9lMajAaAFc+u85AHRtV83xe/TxFRaPvZOZx+jWMD62GsolazazW88OABxl9bq+P24If5i2ECDds/7vO5+lG6M6y2yxcVt9lrAA2Pea5/hi83b+7/hhjB7YjfkrNtK9fTXjR/bh51MyL/5B1z0PwFtXjqNnx0wj9rtnFvBPh2D53TML+MVJI7LOsWJDttnmhD+9kt7+18xP6dqumm8dvCud21UBsGlbtkmjvjF1L86/401mWb02gAHd2qV/f7z6SzbXNbDYYVp42xJQn1qNs7Pn9vT7ueNE/bu1471lG1i+LrsReHnhmqzftjmsu9VQ/uetpXy4YlOO+SyIzZbAamj07sGff+db7N6rI18fMyCddscrizltdD+OuP4FvnD1Qp3vi10/u+G68oThGGPY4xfP8rV9+jF855TAe3/5hiyBYTNt/qr09mPvfMY3xg5k2ryVfOvuWezUsYZVjgbzrtc/4ZjhvfjfY3fPKsPdo1/rqO/UuRmNbFt9I0vWbOaCu97KqUdD0mQ9T4BxN77IuWN24Z6ZvuGSsrA7KQCn3vJauiFuU5VgW32SR+Zkvrmpcz/Pym/jNn+u3Rz+Oc/4MHMvx930YpYJEeCrf3nFfUhJUYFRBiSThkRCctLveGUx0x3mik8D7MvGGC67/5307/Vb63l54Rou+tdsTtizTzp90j2zGda7Y9a4gi0sAHp1asO6Lakeiy0oKq26uRtCIN3w/PqpD7PS/Xo437tvDv/+9gHc+tIiTtyzT854wYsfrebeN/w/5ldr1+Sk/fn5Wpat28pNZ+7F1u25poyrH59Hr041WcICoG+XtulrSxrDmbfN9D0vwNEO09qPH34vZ3/ntimBtTWiLXnL9sZ0jzn0MXWpczQkk/x5+sKchhFgwcpNWb3Vq5+Yx5yl63OEBcD2gPGhpWu30KtTGwAefnsZB+42CiA9RgDwm6fm88CsZTnHVoiwauM2vmWZxFZtym0sn523kmc9xlicrNvibWapa0gy97ONWZqjzZov6zji9y/kpAcJi7AmtJrKCrbVZ9+zB2fnXj/AcX98Oev3319eHOocAF98mblut7BoDlRgNDO2Kahz2yr2G9iVf5y3H5DqOTnt0wBtfMwPAP96I3uQ7uDfPo/97j/53oqsfUHmrPY1ua+E3Zh85urpF8IXm+tYtm4r1z39IVPe+YxhvTtm7V+8ZjNXPvqB7/Hn/OMNz/RVm7axYUs9o65+Nmffw297f8iGjKBev8V7oBtSDXqQ59mzcz/nmBG908InLHe/Hq6X64WtYby1ZB1vLVmXJ3cGP/u2n6YCcMjvZnD47qlQO5UJSQvG/77zGYcO7cmo/l249aVFnseKkNOwRuHBWUsZs2v3LA3Dybb6Riorot33IMLWtTrgW4wTdxvQ3KjAaGaet1TODVvr06r82s3bueq/uY3mZo/es807LlW40GVOkh4HRvVOCuKjlV+m1ei6hkZE4vnYX639gu/eOzvSMcZktKdfPh78YU66x7/se9/4lGNG9KYiosAolLP/PpO2VRX5M3rgNlXa3DyjNvC4Fxakgnk2JE3afg9w+QP+tnoAA6G8nrxYvn5regD/iN29Y8PVNSSJ67YnkyZ0Xf3Gjlo7KjCamaqK3Bdvn2ue88zrZW6xaVMVzwvs1dOsD+h9+uHXMEGmN19VkSAmeQGkxiGiYAyxNPIvfrSaZNJE1jAK5bWPo5mvnPg9lyCvKDfOMad8vLl4Lcf/6eX8GT1Y4zBfzVjgHX16W30jbQoUnjllNeR33f77S4uYt2Jjk2kYheJn5i6W8r7qFkoyaZhq+VvnI8qLZ5shvIjro/H6YArRMM6/8828eaorE7H1DiFYSHkxZ+k6T9t/IWxvTFLpIfzLjcYWssImpMbh8lHXkCxYm3aTNCmNKIhrn5rPo3OWN1nnoFAaCpwLk4/yf8NbIHe9voTv3DObR+csz5u3OkIj8/i7KzwnVhljYtMwvAbWChEYbs8gL6oqEiQKVDGG9uqQkxbV9FHfaJi5aG1B53dT15CkvJuQFFGFanOy3meg20ldfaOnGbUQGpMm9KB3ZaL8ms4rjhuW3i7Vcy6/q24FfGb5VrtdA73wMkn5sebLOo684YWc9IakoU1lPBqGl7dMISapMFRVSMEmKa/voZjB1WLZ3pD0rFO50ZIERpj3rq4hmVcrCIsxJrS2UhNTBy1O9hvULb1dnyzNt1B+V92KyNcYvv3pOv78/MLgTC68PqL6xmTBA4tuvDSMhhgHvZ2kxjAKkxilqlOhbG9M5u3p3njGqCaqjT9x9cabgod93FSdJE14V9gwZYUlyKusuXBaK+57Izi0SaGowGgGZi1Zy+pNdZx6y2ux9N7rG4yvW2PksppQw6iuKHwMo1R1KpTtDcm8MZTsCW/NSUvSMMLMTUkaE5sQTEbQMOL0HIwLp7UirrE5N+olVQLyvXSn/e11+nRu47nPK1RAPoImXUXFT4MpBa9+vIYzRvfPn9GDYuvUvroi0E05KkvWbM47oCxlMMqRL6RMZUJKNmBaCpLJ8I183rKMwYQ0cJWnwMi8X3HOTXGiGkYJCWogVmzY5ulp8dNH3498njgFhvcYRmk+jm31yUiTzpwU26g57b1x8M1/vpW34SoXx5quVggVL+LytmsqkiaaKSmwrGT4+UvlKFSdGkapBuVVYJSAsK+S14B3Ib3eQsNJe+E1hhGnQHLjF2Y8H/VFhkko1DvLzS8dca/ymXvinHNSDEFzT8p9foGb2E1SIfOW5RiG49lVtTQNQ0SuF5EPReQ9EXlURLpY6QNFZKuIvGP9/c1xzL4i8r6I1IrIn8QaERWRbiLynIgstP53LVW948SOmumH10N9c3F0N89S26XrG8KV37aqgpF9m8ZOv6nIQf64PienEMjfcEU76zHDe0WvUCj861Hu8wvc3Pnq4hgHvcO71QbNiWounJ2gUs0JKmV34jlgpDFmT+Aj4ArHvo+NMXtZfxc50v8KfBsYYv2Nt9InA9ONMUOA6dbvsueNxWt5+1N/k0tcvblST8ZqcLno9e3SlhMdAQ1tKhNSVj2vvfp38d0XV2+/zuHKm09gRD1nVcl6+7n1PGhwd974v6MiuXmXAxu3NRRs1nQT5TMKij3WXDjfr6oSCf6SvR3GmGeNMbYYngnkxkF2ICJ9gE7GmJkmJebvBk62dk8A7rK273Kklz3uaKzOHkyUSXtBhOkVtasu3Da95svsCVQjdu7ED4/ZPSdfRYWU1WBgxzb+Ph1xxbByCgm/AHnpc0YsO673w43X6/LLk0bSq1Obkg2WlhLn+iPFEMUkVY60dA3DyQXA047fg0Rkjoi8KCKHWGl9Aafj9TIrDaCXMcYOufo5UCpdHUgt/DNw8pNZi5fEwfaGZNZ4QBQNI8gmGaaN7t6hOn+mkIhA+5pcAVRuHjZHB5h04moWx4/sDaS0Luc6DZ7njCiknM88TLDBsAHxvDQhe1zDaZIqpXnqO4ftGltZcY3hNRbhcbWzj9djU+J8WmXpJSUi00TkA4+/CY48VwINwL1W0gpggDFmb+By4N8iEtrwbWkfno9VRCaJyCwRmbV6tXewsjDc8WoqXv0XX+YPTbC9IcmnrpW9/F66oT99Or2QEESb5R3U2IQZw4iztyoI7atze++ViURZmaQqEsKuPdp77ovLJFVdmeCQIT3Sa4cEEbX9dfYSd3eFgfeiU1t/7ycn6zzMKXbdnO+kLQxLQb+u7WIr6/ONxYfdB9vbqrD395IjB3PBQYMAGPeVXvzm1D1y8owKMJHGgVPDOGyod3Tfos9RzMHGmHHGmJEef48BiMj5wInAOVZDjzGmzhjzhbU9G/gYGAosJ9ts1c9KA1hpmaxs09UqPDDG3GaMGW2MGd2zZ+E3zG70wvSwrnjkfQ69fkZobcRp3okiME73WNnMJoyXSJy26UTCu8dbkZCc8Y7mRBAeufjAnPQzRveLzUtKEKorEmwOMQgfdR6G8/37P2sZ2CA6BZjg8mHXzdkzLaXoj7P/6zb7RmHfXTL+M+NufJGPVkaLeGyTEEnfu4qEd+eg5MY+xwlG7Ny5JKcopZfUeODHwEnGmC2O9J4iUmFt70pqcHuRZXLaKCJjLO+obwCPWYdNAc6zts9zpJcE2w5vvwD/nbOcET97xtM+/9LClCazzeEOG9YSGmVQ86LDduN7Rw723BdGYMTpLikIiYTkjItUVpTXoLcIdGlX7WkuCCMvwtyzhKSEcZhV9sKc80fHDM3kd6TvP6gblx7h/fxtOofUMLyw69ZUQfXKxcX4we+M5c9n7Z3+/d8QAUNtnNeQkEwPvzLhH1TzkiN2K6yiIbCF1JCdcgNzxnaOkpUMfwE6As+53GcPBd4TkXeAh4CLjDG2L+nFwD+AWlKahz3ucR1wtIgsBMZZv0uGbYe3H/ovH5/L5u2NbHT0ZJJJw4wFGUWnkGYyipmxpjJBG5+B6zDjBlUVCV6bfGT4EwZh1budyyzVVGMYUc1rXua8ML39UI9H4hXGew/I9HjdtzKfwhuHwHCOm5SyTS+HWe8AiYRkXXOUt9epASZE0r8TCfEUGCJwwKDuBdc1HyLC21cdzZRLDy7ZOUoWGsQY49kdMsY8DDzss28WMNIj/QvgqFgrGICtSdg9d/slcjY8/3xtSZMun1hZkfA1kYUZw6iqELq1j2fge9O2lPmlQ00FaxwafGoMI6OF7T+wG6P6d+azDdtylokthqi9U69Oc5gywuRJiASa++48fz+++c+3QpfnfMZuTTXfoHnYMQwvnL3jpqAcNIy9B6TGFAr1mKtISDqUTkIk7ThQId6TI4XSXndCiO0b9z1HSUtvodhmFdvSY/93Puul63LXpUgdm+TOV5fkpHu5vkZ5USsrxPdjDmMGinN1O3slNLeGUeHSMKorE1x5wnBuPnufeE5sEfU6vHqzYe59WC0kSMPo7TCHhRk3CXKHzHd8kIaR79RpgdFEbrUCLPjV+Lz5Sl0HgArHzYniJeU8LpHICPuE+Iftj2vszIum0NpUYHhgD9za747d2Id51k++792TvmfmJzlpUR5vpUt1dhJGw6ipDF6s6Hdf2zN0XeyzuV1rqyqyBUapwhNExd3Zk5CfVlgNI8jl1VlGVA0j91zBx3ZvX1NQueA0STWdhlET0xouxeLsh3lFZ/C7dQmXSapdTaoDJeJtkgoSJGHZZ0DpJ6MGoQLDA1vNtP270yapEM2MV7TXzzds4+rHizNfVSYSVPhpGCE8k6oqElk9IjcdInjY2ALUS8NwCq+ok4e+PmZAqHxRe1LeH2+Y84TII8FzXJx1zVfvSpft293bzbdG8zfG7uK7L1/P1t5dyh4wwH4DU2M05TKGAcHa5k/GD2Ngd2/XbPcYRkdLYBiMb7yuYq87KA5YUwgMDW8eBccDcX/MjUnDwMlP0rtTrkfO2OumFx2CuapCfM0F59/5Vojjg01SQS9iUJlOKhMJjhnei2fnpXppkQenQ35MkT8MLxfHMCapkHl6BAmMCBpGxzaVWXncr0y+44MizeZ7vva9L3UoqV7291EG8sK+v0FCsku7Kt+BcOc9TRqTEejG+z42GlP0/Q2qa6mFPaiGEUi6kQ/R2NsD5V6TiPyERZTnKw4vjELIt7pdkPYRlsoK4U9n7c0wa5JZqYIixvFZhNIewuQR6NnR3xQkPttedGpbVVSDELQ737H2q1VIxyEKYRrppibokhPiH3rHeQ3b6hvTzzdpjOf1JZMmr5aYj6D71hR3VAVGAG4vqUKIK5ImFBcfproy+HWqiDDekLmk7GurSAhtqirSi/Q8M/fzKFUMTdjGJkjgh3I48MjiFtoJEUb1C2dXzuvl1KYqW8OI6FYbVHy+q7Xr1lQNefmIi+DOkiC+37/zsLqGJPtba6ycMbq/p+BNmuKvO8iJTTWMZsY96L1lewP/eHkRyaThizyB5mwenOW/LnFUe2YxESjzDWbGETeoqUJjR43i6jWxsUB5wU1n7pWTp3sHfw3DWUq+c3ZqWxlJw3A7FYR9n/bslzsL2H50pWhznCY7u/hyUDAydQmWtL4WAsf93lbfSP9u7Vhy3QkcOLiHt4ZhTMEuvB2s8ZFADUMHvZsXt4Zx7ZPz+dWT87nhuQU8/u5nWXn9Xqp1WwIES8QHXIy5YP88K8xFMUn5zWR3a0Bhg+HZhK1C2LERuzyv2oYzN+Xmal9Twa49M4OgUcxE+c7ZvrrSpUUEz8NwdwICXw/HPq+w73bj5/eO3XPh/gGFB+PlDVUOAsMm2CQlvu+787ht9dmOJ16mp6RrbGPMruFWfbzpzFF0sVZJDBYYqmE0K8fc9BK/e+bDtDCwY9bM/iQ3/r6f2hrnUhXFuDwGrQ0B0YSRe36KjZe5phRU5TGvuXHXM2y1vPIlRLIedr6yssYw8mSuqkxEMklFud/usZRT9u6bvT8RXMYhQ3qyW09vb6F8eDlrlJOXVNC4QqAMdo1hOGnvEZXBeGgYYTwD21ZVpF23Sz3GlA8VGAE0Jg23vPBxuodhaxxeg7l+YxVxCoxiJlUFDczaZf/6lNwIm0G4L81+me01o+saoi83G4aogrPQJTy97ra7Qc0rMBwZ3FkHds+O2JoSAOFNUhu3ZQc8DC8Ihd+dtidv/l8meEIYU1GUHuyNZ4xKb3vOei4feRGoYRj8v2HneMLY3bJDfuy7S1cmHzcsK61nx5qcc/3q5D0Y5WEizCYzi7y5F0RUgRECd4/aS2D4OQQFNVZRn32hvYtzDhjgaRb448SMPT4hEjoWkd8V2T3e6T88HMiOBBqGsFdXVWT4CpeS4ItXb7sikT0ImtckFSFvaiZ/pvTog97hzRVVFQl26pQ7C91ZR/cxUd6+Ho5xnTg88EpJvufi9wkvXbsVgNG7dOWQIdnRsUWEc8dkz4v548S9PefZ5FsxUyTz7TeF2SkIFRghsB9nWsPweL6+GkaM9SjUJHWtj+bgDDWSmhjoX0Z3R4wa+1rd12yPYXRrX83Tlx3C7efvF6p+x43sHSl8SD6T1Amu5WPdjybo+3R+5F7fpkj2def3PnL+CM6bfzZ2YY3FKXv3zTsfxE7LDpORfaOimBidnZtizCiTDo1voSU3YTzDwng51lR5fzjucv3iPOWbdyvgiFOlAqP8cWkYXit8+WkYQXMRoj77uL2QnCauioT4ziSHlHfQ/ZPGAPk1DICv9OlEpzbhNJZDhvTMaeT3DgiB4DXobdvkzxjdLz3j1iaKa/PQXpnQ0F4NdGoQNPt3EFkzvfM8Prdrs3uwtdAxIfdRXuMHaQ0joEWINm8os+18z6IKve4lDqYHwZ26lEkqjwbg0xPwHQNzkX8t+EwEAB3DaAHYH679WL1CePs99FjnYcQcSdTZW6lIBE8MTDhe2rQAdZdX4MvsFdokqC5e81G8TCo2XjOm/SdT+o85QOo5O4M95h/DcGwHnAty3aajmqT8K+Ffp5ysATvt+TVhcGrDTRUB18ngCGtCNORZ4zjf/FO/W+YpMDxuRV6BgdMkFVyXUqMCIwK2ZuGlYfhpEkGvQtiJ0E/8Tyq+fdyRRJ0vb0rDCBIYmQbLr9qFLp7kdVxQXbp4jLVUBth4owx65/sg127eniXgovSY8+UN0vCgOK+zfJaxdA82phapIiH85ey9uefC/YvSjDuG1FLddGsXXjPx6gD2saMMm8KdJvwCELoJI5Ds56JjGC0A4+pRe/WI/d6poJctrPYxsm/KiyJuk5Tz5atMSE70WXdeO79fvQtdKtNL2AaN15x9wAB+PH53PrwmEx7bNud43aIo37vzg3Z/3JccsRtHD+8VKeSJs4h8j8/dIfCLJdXGYTP/97cOCFmPTNm7eKxzbu+O6xWrTAgn7rkzhwzpmSX8o2jc3zp4EGfu17+wCkS4Dq8Oi/3sDSZ0xy5MFTy11jwnEMl07uK0WBSCCowQJF2DvF7P108wBA1ohWl4vrZPZi3vQkKDuDskZx+Q8fvu16VtersiIXQN6JUlJLfX777kifsX9nHXWzfJ2agFaRhVFQkuPnxwVrC9ygCVPconltUTd5X1v8cOo6ayIvSqgm9dOS7b6yhPK5bPpGcLlF17ZMwtBw7uEaouNjecPoqvH5Dr+5+IuQfrvJZLPZYWDnOe8w4c6HtPHv5u7lrtTvIJvon79U+7/notvbw0xPgAACAASURBVJy2vsasYXhdd14vKYdbbTPLC41WGwW7nfB2q/URGAFPOIwF5waHP3sYDeM7h+3KrS8uAmDuL4/N2f/rU/bgVxNGsmzdVgY45gFUJISObQIERkJyTFLu6neJYAZw0uhxI4I+DK+2JuOnLjk1K1TD8MOrvl707FjDig1b07/zFZ0v9IvfWMC1p4xkyE4dA4+1Sz5ocA/PRstOcTbQxQgPZ13dLqeQ21M+fo/ePPV+duyxoNPnc9nOJ5yvc6z/Ys+i9jt3oUE0vQe9c9Pyli+Z97JQ4RUXJdMwROQXIrLcWs/7HRE53rHvChGpFZEFInKsI328lVYrIpMd6YNE5A0r/T8iUnrXCQf2I9qyPTVRKso8jKCBwnyqqJswYxg/Omb39Hb7mkra1+T2CRIJyRIWkBJGQV5NzkXu/d7ZQk1mXj32oDU+vBoD28br3eDnGHf8K5NlQvLOVx9QN/dclqCGy73HPYbhvs9+C1Kdc8AueUO/5DM5ee0vxvzhux6Ezz09dkTvnDSv+//a5CN552dHF1wvL5zrqLvPbShi4mfWnJbcNJt8xYujPmG121JRapPUTcaYvay/pwBEZDgwERgBjAduEZEKEakAbgaOA4YDZ1l5AX5rlTUYWAdcWOJ6Z2E/0I9WphawjjLTO4ioL2IYb5NCB0YTCSGREN+V91JjGNlpufMwChUYuQ3w9oYAgeGlYQSc2+s2+8UHCnMFQT1C20EhXZ54b9scMiRjUqqskMx6ER7E4m3kKzCCBK67HvHkcRI0G/yHRw9Np3VrXx1Kky3WsubsHPl17CZGGF+pvfZ4q9zcfXY78MuTRngeK5IxSZVqyYCwNMcYxgTgfmNMnTFmMVAL7G/91RpjFhljtgP3AxMk9SYfCTxkHX8XcHIz1DuNl82xkAcZ9ZgwbquFDlraH/gZPh+B063WN/hggQ3auWMG5qQ5VzlzL0vp1agFNVBRBHPCo1foJqiX179btuaW7Z2UW+A9Fx7AqdYcksqE0LFNFdeflhLa7vscJQR9Li57og+JECapn391uGe6k6gu1l7Pz34W/3PUEEedMvsfumisb3nFxjFzHu33uE8fnRpfDBLyNtkm02zsdsBeR8arLq3eJGVxqYi8JyJ3iIit9/UFljryLLPS/NK7A+uNMQ2u9BxEZJKIzBKRWatXr47zOrKIYpIKIurDD/MJFGp3zveBl8okdejQnvTunP3BXXXi8Kw0tyDyHsPwf5UjDXrn0Qgg4sBjlonLO4vdAckX/qEYl9ewh4Z5hBP3zx8wL6q26fX8PGekO27o6IH+ZjivY6MET8xMOTK+3+m+u3TjpjNHcfUEb80gLHbb4XfPRDLfVjMrGMUJDBGZJiIfePxNAP4K7AbsBawAboihvoEYY24zxow2xozu2TN3oC0uvCb6FCL5bRNXWIKW4CyW/ALDMejtJzAK6AGHOWJU/+zgbM5jvmOFjqgOOLenScHnGuKOsJs909u7bLsDkiNwDTzwnbHca7nOxuFWne8tDXP9YQRXHBqG94x0/zK8IsTaXHPySH5/+ijf/bnnyXSOgj7tU/bul7O2fWC5nmHP7Q6Dd3MsCB3a2OthhD5VSSjKS8oYMy5MPhH5O/CE9XM54LR79LPS8En/AugiIpWWluHM3yxEcauNwt4DujDn0/W++9tWVzDjR4fz4oJV/OLxeUWfz0m2d4x3SHB3g5c7KOvf/0hI6r79z5GDWfNlHfe9uTQnj9es6EuO2C3LPJXKl8k4+bhh/GT8MG59aZHvuYPOE7QvDuERNNPbTkgLDOv+2fkM2euYFLq8Z5RQ4s6cg1yOEVHqEdU86VWm12nCatDOfHv07cy5Y3Zh+fqtAUe4j89su03QPxg3lG4B67gHEeQlVZk2W2W3MSIprXvnzm3o2akNLywonfUkH6X0knIGBzoF+MDangJMFJEaERkEDAHeBN4ChlgeUdWkBsanmNTI6gzgNOv484DHSlXvMMRlknITpgc5qEd7qj0izxZ/7syrYDeU3zxoIL061aTT8lUveKZ4at9Jo3bmN6d6D6zbOAfTvYRQttlIslx+PcsLPJu7bIfgjHDcvy48gBs8erBZYxh+JilXg+GXr5jwEGkhlO9mWIXvu0tXvucYO4hKoRqG+9nmVM/1+6X/PSK9bTzy9e/WlgetsY4oVXJ6STm/9/u+PYbLxg3JiUQbFi/hba+m5zfOIaS87y4/ZvdWHXzwdyLyvoi8BxwB/ADAGDMXeACYBzwDXGKMabS0h0uBqcB84AErL8BPgMtFpJbUmMbtJax3XjwFRgwSI/Ra1bHGwLXPndm2X8oLDhqUNoNlDXqnVyLMrkdQ2PFEukEowGzl8QHllB9QrreXlM+5CHaBBDhhjz45aQcP6cHX9u2Xk57tWuldnq2dunvZbi+0YgLPXXPySHbu3MY3YqqbQ4b0KGoNeb/Oj98V2Pm7tasOdAF230K3e7g7X5e21el3OJKm5Zy556BYk5DX8f/+9gFcdeJwOlku2TmvSYhxtaaiZBP3jDHnBuy7FrjWI/0p4CmP9EWkvKjKAi8vqThMUmFLaB/BZhqWrMbM/mATGSEhkl+gBXnxJNKNsPu8ztOG+xq8Z9D654/i8pyQVC1MQJl/nLgXT76/IlR5Ya7Irl6F4157UUzv8tgRvT3nOuSrU6H4CTe/Ym3h1GgMVYkE2xuT3hpGwD247Kgh/ObpDwGy3tvMsSEqbuf1SS/ULJipQ+7xu3Rvz4UHD0pP8sy8gTh+lwc607sASmWSCstJo3Zm+fqtXD91QUnKTzfuru2M54g3+aLd2uU48T3C8WG1cw1menrPBGkYvnsyDN6pA7Wrvgwec7CI0vMOHC+x/tv1cyto7no3RXiIuJqmqAP09qRE57cVtoGfdvlhLF+/lcOG9mTD1npueeHj0HGc/EjHbnKnF+uu6zg8dzqpd4ch65iW7CWlZIjLPzrMd5ZISNo7qBRUeGgVhkzvKhNbK/u4oEai0GibxsD4Eb35+VeHp80pXj2uoFLDPBtbC0lIpo5xxFUK0zvMnNsWqt6CocJ1/8uZ6GMYloaRNGlTZ9jGefBOHThsqHvFO+t/VmL4+vi5kBdvkvIvIGOKy2+CbS5UYMRELFEkDcz6abiwB8XYl/PhnPXrnDBUzKB3kF3aM79jO5EQvnnQoPTgYL52pK8VVNEWMNEejXhswbiv9IpSiFdxvtjVE4eg9sK+v00RHqLYM0QVtraG4by2IvvyOSlRtANfk1SRnYig91988niFGHHym1P3KHgQPipqkoqJPGuwhCbK61h77XEMvvLpeE7srIOHGSqZzO355rreBpik7EFv1xXm+xicp8jnRWTz3cN3Y0ivjhwzvFdWGeO+shPT5q8KvMcJyTwD53n+cd7o4JP6EKZ9se+jxzSMLNIaRnPP3ioB6TGMZKZjEv+cmAh5XQ4eNsXPIM8vMXKcPPKc8qwQEynjQjWMmIhn0DtibKkSaRkVjoELp4bhHsOIUt98A7puvPJV+AgdN5UVCcaP7O3wdEn969GhJp3HTyPMdquNwySVn6TxNsG467hLt3Ycv0dv/jBxLyAVDbdURL3yE/fM9RyLUq7dGWhMmrQALa5tzi0kitbjjsycTo/4ye27S1f+7/hhmToELYHrN4bhkTdKHKs4UQ0jJuKyK3dpV8Xp+/bjwdnLYimvEJy29MzCLf6+/N3aV7N28/bgMn20gxE7d8rJm1W+40d60DeiYE3bxAPsAemBZ3G61UY6jSdBjVQYd2EnlRUJbjlnXwAevfhA+nX1dimNg6hv869P3YMT9+zDyo11DOgWvV755tuExe0N6yyiEA3DTVQNI9+6HdllW+fOqUtmu7mHr1RgxEQsQxgm9aJef/qoZhYYzm3HQKvPt/KniXuz36Dg9QkybrWpjSXXncC7S9enVxME98ede7JKD0+aMNjZnW6pOeMAtlkoITlujcUQ1Ly4NYgoPWCvkNy+dYjQxoW96j6d27Biw7bMOYDxI8NpGV54hZUpRsPzuo4ojb0zDM7U7x/KsX94CfAPMR+6XgGma//nXz7D3mqSiomW4LnixYwfHc79k8ZkpTnttxccNAjA1WvM9pJKJKAmz+xzr491VP8uWQPlB1uhvvdxLI7jvKt2rB2vQd9tDY0A1FTlvtJHDtsJgHaOJWhXbtyWlScdzyekCWOgz4QxN4WMYRTjnbVnv845aV5roPuxfF1qLkCfzsERWKddfhhvX5Vx0CjWo8xr0mc8Gp7zR5TjbG0WdndEkc237no+guYq+YoL8d5uDlTDcFHogGI5jUPeP2lM3g/eZlCP9gxyrfFsN1xJAyfv3ZeTrfDbqzfVAR6D3SG+xLSmEnCjDt99Jz68Zjxtqip4ZeGanP1OO7ebL7elghl39Fgw6uaz9+HDzzfy7tJUnC4R2LStIStPWgtxjHofsXtP5q/Y6FnXpy47hK3bG32vxSZwAaV0o+QzhpG39FxO3LMP7y3bkP597phd+NGxuwcckU3fLqn3Zm9XSHk3qcW5Mr+LbcecGsYDF43l0beXFxVs0WuMKo6Je8UGgOxQU8kPxg3lpmkf5Z7Tp+jy0S9Uw8ihUE2hnDxXxuzanV26hw/l7MYv9r77hY5yxXbHLN/9TYdxCBj0bvBYItUWAB09Vg2srkywZ7/sBvAvZ+/D98dlYiV5DTx7hfqwaVddSfcO+QedI2kY1j1KHxLDK3Xu2F0iRTm+9MghPPW9QxjWO3dsKYhivYecAmOfAV255uSRhYWRsT350r+d+8Jju2b3cAUZLHSRMCcHD+kOZGJI2fh1Lpz3obkNGSowXORbkN2PpgwNUmrOtDwwOrlMGdWVqdflK32sxiSCN0tGCEWri/O2BmoYdSmB4f4I/Ri8Uwe+Py6zkpvTLGRfTlP17DJeQdE8yYLKKpTqygTDPRwR8lGsqSQoDlkUghwigoTag67FmC44eBC3nLMPJ43aOSs9jlUP9xnQlf89dvf0Qln5KCcNQ01SLgr94CY/8n4M586cfOYVR6Ub6KbmsqOGcPHhg3PO36lNFQ98ZyzD+mSvDBbmhbbHBsIOWHuVaa857vXd9++a6hHuEnJswQ/nmErc4c398DdJlUsXIsVjlxyUY8qLi2JjNLlxC+HUtn/+/VyLMVVWCMd7BJmMY00SEeGSIwZHyO+93RyowHDR3CqfjXsVuqZERKiu9H4zneszRGnQbOETdUa88xy/PnUPdu/dkbG7ds/J9z9HDeHAwT0CV2ELOrNXxNhYBl0DxOlIqyfvHvQes2t3qisTfOuQ6OFf3NcY5/s8qr//uEZzN2RhiLYuiHfeOExSUfGry9kHDIhFgEVBBYaL5uzVlYmsikwYW/Ot5+7Lv2Z+wuCdOoQsMzetW/tqfnD00NwdpPz4x3gIEs+yPT5Ap5dUZqZ76TSMh797YHoOSmYOSCpzjw41fPSr44o+d1NSThFV/Yg06O2TNw6TVFT86vLrU/Zo2oqgAiOHMhq7blXs0r09V54wPPJxTaXxpUOMx65heLOv03XYOnkc53Pfr6bqAMXV0S12noMtuOwOQKGl+XpJNYOGUU6owHARSxDBZuAvZ+8deY3wYmlpt+q0ffvxzqfrPbWUZNosJEX3lvfo25n3l6dcW8NoX8Zx7mJpLg25kCjEbv514QFFj0G5cVYrUvBBXw2jGUxSjlPaATWby2StAsNFc2oYxTTAJ+65c/5MMZOJstrkpy6IdtWV3HjmXp777I5CHBrGQ98dS32jfw+3U5vsz85tkmqJ5Kv561cciSD85un5vnnsiZulItrtze/iGjcd21Sy38CuXHzEYL5551vpdGe7cMzwXtxyzj4cPbzA6MlFUjKBISL/AewZQ12A9caYvURkIKklWO3Vf2YaYy6yjtkX+CfQltTKe5cZY4yIdAP+AwwElgBnGGPWlaTizSkwmu/URVGKT6iUH6YX9r2vSDhjSRVWh5rKCmzvXncRT37vYHbqmN07dK7FUSw5Jqkmeqny3ao+nds2TUVcOLXFKLe3OWR3IiE8eFFw7CkRb++tpqKUS7SeaW+LyA3ABsfuj40xXl29vwLfBt4gJTDGA08Dk4HpxpjrRGSy9fsnpah3Sw3x0Rz88qQRXP34vKx4UHHTVE/Da+JeHG2GW+iM2Dn3XjWnpvbwdw9k9aZt+TPmoakFfD68PuModSynqymnJqnkJilJPaUzgCPz5OsDdDLGzLR+3w2cTEpgTAAOt7LeBbxAiQRGGT2bsmdk38484Jrw1FKxZ+pXJDJ90qZqAzMNQvEnDDtx0cY5+N4asJ9ZeytuWN+uGc0migZXTgKwnObjNMUYxiHASmPMQkfaIBGZA2wEfmqMeRnoCzhDtC6z0gB6GWNWWNufAyUz4KmGUV40tZdUtobRNI1GnBrG2QcMoL4xyb/f/JRFqzeX/P49/N0DeeaDFfkzWjTV8xyxc2dOGtWXw3fPLN1aTkIgCuXUJBXlVCwi00TkA4+/CY5sZwH3OX6vAAYYY/YGLgf+LSKhYxGYlMHX8xaKyCQRmSUis1avXl3AFZX24eycz7OhnN6MZqapv+3MGIbEEqIj2smLcwF1UlWRmvBnT27s3C58pNpC2HeXrgW5SzcFJ+zZh/YRNK4fjPOe49McHGVFWIbysnoUpWEYY8YF7ReRSuBUYF/HMXVAnbU9W0Q+BoYCywFntLd+VhrAShHpY4xZYZmuVvnU5zbgNoDRo0cXdJ9Lqf61rQ4OAlfImX/+1eElXd+7uSm1Ov6vCw/gk7WbufbJlPdOlkmqpGfOkNEw4jvjz786gnPH7pIOolcu2G6h+b6FQrG93ILWl/fjsnFDeHTOMpZ8saXZ3etvP38/JvzlFd5dtqHZ6+Kk1CapccCHxpi0qUlEegJrjTGNIrIrMARYZIxZKyIbRWQMqUHvbwB/tg6bApwHXGf9f6xUFS7lsymF2+Q3rfUqWhtfsaKleg0Sx8nBQ3pwMD245ol5gOsZNfEYRpynq65MRI4468XbVx0da4P1k/HD2K1n+/R663Hz7UN2Zd3m7XzzoIEFHe8ntF+dfCRrvwxeVTJ2HGtylAulFhgTyTZHARwKXC0i9UASuMgYs9badzEZt9qnrT9ICYoHRORC4BNSg+gloZQCI18wwTLqSDQ7Rwzbied/eBi79gwXSqRYsuI5xRgaJNS5iW+md9zYGkFctK2u4NyxA2Mt00n7mkp+OWFk0eW4P8W+Xdo2ubZmvw7l1C6UVGAYY873SHsYeNgn/ywg52kbY74Ajoq7fl6UctDba91ixZ+mEhaQPYZh01STejMaRhlKjB2McmqkMx2IMqiMhc70dlHKR1OtAqNsSU+ey5rp3UQaRoR1RVobFx++GwtXNW1Im0DK6BmUk/CyUYHhopQr51VVCjN+dDgfrdzEd+6ZXbLzKNFJL9Ea88S9MJRRe9Dk/Hj8sFD5Zl5xFGu+rCtxbZw0/1ORHXAMo8UyrHdHPvx8U6xlVlckPNfQtimnCTo7Gs5YUqWYuHdawHKvcUarba307tymSQLulVOvvpzqYqMCw4XXSl1xkW8Mo5xejB2NrGi1MT/7Rb8+PpQw0DEMxYn9zpSTW60a1V1kYgrFX3ZzLbmqhMc56G0MdIwYasOLRCJYCO3IYxjlRjmZgewORDnUxUY1DBelDDWtg97lS9d2VazbUk9Cshvut346ruThYsrZrXZHo5wewQUHD+TNJWsZEnKVyqZABYaLUmoYfiap/0waw5m3zVSTVDPy6MUH8ebitVmagAHaVJVmRrITdattOg4d2pMT98wfHrwcvsXxI/uw5LoTmrsaWajAcFHSMYxK7zKjxLtRSsPAHu0ZaDkjNHWz3dIWomrJ3H3B/oH71WwcjLZULuJczMZNvgXky6BTozhoqsHGUr5zSjT+9vV9ufeNTxnaq3zMQOWEilMXpQgEZ+NXZDl6Q+zIXHrkEAA6tGma/lSc62EoxdG/WzsmHzesxYZCLzWqYbjIiikUM34D6Wq7Li8uPHgQFx7cdEEd1SSltBRUw3DhtVRnXGh7oHiRnrjXzPVQlHyowHDhtfJajw41sZStPUjFiwl7pRaW7N4+nvdMUUqFCgwXaQ3DcWd++7U9Yilb7aKKF5cdNYT5V48v+ep4ilIsKjB8cGoYcZmn/ErJDHrHchqlhZFISMlWoFOUOFGB4SKZDgQX/8prfhqGKh6KorQEVGC48FouM672XP3sFUVpyajAcJF0hLkulIe/e6Bnej5NQsObK4pSzhQtMETkdBGZKyJJERnt2neFiNSKyAIROdaRPt5KqxWRyY70QSLyhpX+HxGpttJrrN+11v6Bxdbbj7RPvEdaWHb1We/Cb76FzsNQFKUlEIeG8QFwKvCSM1FEhgMTgRHAeOAWEakQkQrgZuA4YDhwlpUX4LfATcaYwcA64EIr/UJgnZV+k5WvJMQRSyrfjG5FUZSWSNECwxgz3xizwGPXBOB+Y0ydMWYxUAvsb/3VGmMWGWO2A/cDEyTVQh8JPGQdfxdwsqOsu6zth4CjpEQ+qpmV1+IvO9+gt2oaiqKUM6UMDdIXmOn4vcxKA1jqSj8A6A6sN8Y0eOTvax9jjGkQkQ1W/jVxVzqO9TD8Gn6/YZHBPTtwwUGD+MbYXQo+p6IoSqkJJTBEZBrQ22PXlcaYx+KtUuGIyCRgEsCAAQMKKiOZLN1iNr6CJCH87KvDPfcpiqKUC6EEhjFmXAFlLwf6O373s9LwSf8C6CIilZaW4cxvl7VMRCqBzlZ+dz1vA24DGD16dEEuR/ZBHYpZo0LHMBRFaYWU0q12CjDR8nAaBAwB3gTeAoZYHlHVpAbGp5jU4MEM4DTr+POAxxxlnWdtnwY8b0oUC9x2qz15r76ZxIhn8h30LrBOiqIo5UAcbrWniMgyYCzwpIhMBTDGzAUeAOYBzwCXGGMaLe3hUmAqMB94wMoL8BPgchGpJTVGcbuVfjvQ3Uq/HEi74saOHXywBLPsSlGmoihKU1H0oLcx5lHgUZ991wLXeqQ/BTzlkb6IlBeVO30bcHqxdQ2D56B3xHZexYKiKK0RnentIhNLypEYk/HLy/PqlZ8cEU/hiqIoJUYFhos4VtyLEmSwX9d2hZ9IURSlCVGB4SJpvIKDRMPvSB3CUBSlJaMCw0VmDCP+snUmt6IoLRkVGC6M13oYEZjxo8M1lpSiKK0SFRguvNbDiBJ2vH2NrpymKErrpJSxpFokmUHvYHWgMiE0JHMFSWUiERBLKpP+7A8OdYyXKIqilD+qYbhwutUevntP33z3TRrjmV4h4mt6co6LDO3VkWG9OxVcT0VRlKZGNQwXaR8pgdvP24/GpOGV2tU5+WoqvWVtRYW/ZlKiiOyKoihNggoMF+lBb4SKhPgu1epnsqoIEAoqLxRFacmoScpFegwjz53xa/yD1gJXeaEoSktGBYaLZNpLKtO8e41N+2oYCf8xDFUxFEVpyajAcCECVRWSd+Ken8AIOk5neiuK0pLRMQwXx+/Rh+P36JM3n//kPAnlVqsoitLSUA2jQBo95mDY6AJKiqK0RlRgFEiQwPBDFQxFUVoyKjAKZEB3/7DkbrnQv1tbQEOZK4rSstExjBAUG8HjgoMGsVf/Luw9oGs8FVIURWkGitIwROR0EZkrIkkRGe1IP1pEZovI+9b/Ix37XhCRBSLyjvW3k5VeIyL/EZFaEXlDRAY6jrnCSl8gIscWU+e4CLIuuWd0J0RUWCiK0uIpVsP4ADgVuNWVvgb4qjHmMxEZCUwF+jr2n2OMmeU65kJgnTFmsIhMBH4LnCkiw4GJwAhgZ2CaiAw1xjQWWffQRB17cGfXsQtFUVoDRWkYxpj5xpgFHulzjDGfWT/nAm1FpCZPcROAu6zth4CjJNVVnwDcb4ypM8YsBmqB/Yupd1S8TFJR4kKpvFAUpTXQFIPeXwPeNsbUOdLutMxRV0mm5e0LLAUwxjQAG4DuznSLZWRrK2XDqXunqpUjS1TFUBSlFZDXJCUi04DeHruuNMY8lufYEaRMS8c4ks8xxiwXkY7Aw8C5wN3hqxx4vknAJIABAwbEUaT/uTzSbjxzr9B5FUVRWhp5BYYxZlwhBYtIP+BR4BvGmI8d5S23/m8SkX+TMi/dDSwH+gPLRKQS6Ax84Ui36WeledX1NuA2gNGjR5d0daIgpcFtrlIFQ1GU1kBJTFIi0gV4EphsjHnVkV4pIj2s7SrgRFID5wBTgPOs7dOA500q1vgUYKLlRTUIGAK8WYp6lwq/UCGKoigtiWLdak8RkWXAWOBJEZlq7boUGAz8zOU+WwNMFZH3gHdIaQp/t465HeguIrXA5cBkAGPMXOABYB7wDHBJU3pIOTlocPf0dhQhoBqGoiitgaLcao0xj5IyO7nTfwX8yuewfX3K2gac7rPvWuDaAqtZNOlV+ArUFFReKIrSGtDQIBGwNYXANS8CjlMURWnJqMCIgD0fI2gZVi90DENRlNaACowI2HJij36dm7ciiqIozYAGH4xATWWCBy8ay9BeHaOZmVTBUBSlFaACIyL7DewGQF1DeEctlReKorQG1CQVAuMRTCqaW62KDEVRWj4qMCKhbrWKouy4qMCIREbTULdaRVF2NFRgFEikMW8VGIqitAJUYIRg157tAThkSM+Cjk+oxFAUpRWgXlIhGLxTR2b9dBzd21en03QgW1GUHQ3VMELSo0NNJCHxt6/vw+CdOgAqXBRFaR2owCiQfCJg/Mg+9O3SFoCEygtFUVoBKjBKSLFRbhVFUcoJFRgFEsbKZE/4U4uUoiitARUYBRJlXEJNUoqitAZUYDQJKjEURWn5qMAoIXYIKtUwFEVpDRS7pvfpIjJXRJIiMtqRPlBEtjrW8/6bY9++IvK+iNSKyJ/Esu2ISDcReU5EFlr/u1rpYuWrFZH3RGSfYurclBjsMQyVGIqitHyK1TA+AE4FXvLY97ExcP+xuwAAC4BJREFUZi/r7yJH+l+BbwNDrL/xVvpkYLoxZggw3foNcJwj7yTr+BaBrWGouFAUpTVQlMAwxsw3xiwIm19E+gCdjDEzTcqF6G7gZGv3BOAua/suV/rdJsVMoItVTtmTNkmp4U9RlFZAKUODDBKROcBG4KfGmJeBvsAyR55lVhpAL2PMCmv7c6CXtd0XWOpxzApciMgkUloIAwYMiOky8nP7eaPp0aEmJz1tklIdQ1GUVkBegSEi04DeHruuNMY85nPYCmCAMeYLEdkX+K+IjAhbKWOMEZHcVYvyH3cbcBvA6NGjIx9fKEd9pVdwBpUXiqK0AvIKDGPMuKiFGmPqgDpre7aIfAwMBZYD/RxZ+1lpACtFpI8xZoVlclplpS8H+vscU9ZkvKRUYiiK0vIpiXVdRHqKSIW1vSupAetFlslpo4iMsbyjvgHYWsoU4Dxr+zxX+jcsb6kxwAaH6aqsyYQGURRFafkU61Z7iogsA8YCT4rIVGvXocB7IvIO8BBwkTFmrbXvYuAfQC3wMfC0lX4dcLSILATGWb8BngIWWfn/bh3fMrC9pFRiKIrSCihq0NsY8yjwqEf6w8DDPsfMAkZ6pH8BHOWRboBLiqlnc2EPeqtJSlGU1oAuoFQEPx6/O4cM9l+FT+dhKIrSmlCBUQQXHz44XEaVGIqitAJ0SlkJsQe91SSlKEprQAVGCUmvh9HM9VAURYkDFRglJO1WqxqGoiitABUYJUTDmyuK0ppQgVFCMhpGs1ZDURQlFlRgNAkqMRRFafmowCglxp6418z1UBRFiQEVGCVEB70VRWlNqMAoITrTW1GU1oQKjBKSWdO7mSuiKIoSAyowSkhGw1CJoShKy0cFRhOgGoaiKK0BFRglxDTZIrGKoiilRwVGCdGJe4qitCZUYJQQoyqGoiitiGKXaD1dROaKSFJERjvSzxGRdxx/SRHZy9r3gogscOzbyUqvEZH/iEitiLwhIgMd5V1hpS8QkWOLqXNzoIPeiqK0BopdQOkD4FTgVmeiMeZe4F4AEdkD+K8x5h1HlnOspVqdXAisM8YMFpGJwG+BM0VkODARGAHsDEwTkaHGmMYi615yjK7prShKK6IoDcMYM98YsyBPtrOA+0MUNwG4y9p+CDhKUlOkJwD3G2PqjDGLgVpg/0Lr3ByowFAUpTXQFGMYZwL3udLutMxRV0kmbkZfYCmAMaYB2AB0d6ZbLLPSyp70xD01SSmK0grIa5ISkWlAb49dVxpjHstz7AHAFmPMB47kc4wxy0WkI/AwcC5wd4Q6B51vEjAJYMCAAXEUWRRqklIUpTWRV2AYY8YVUf5EXNqFMWa59X+TiPyblHnpbmA50B9YJiKVQGfgC0e6TT8rzauutwG3AYwePbrZXZSavQKKoigxUjKTlIgkgDNwjF+ISKWI9LC2q4ATSQ2cA0wBzrO2TwOeNym/1CnARMuLahAwBHizVPWOE13TW1GU1kRRXlIicgrwZ6An8KSIvGOMsd1eDwWWGmMWOQ6pAaZawqICmAb83dp3O3CPiNQCa0lpJxhj5orIA8A8oAG4pCV4SIFO3FMUpXVRlMAwxjwKPOqz7wVgjCttM7CvT/5twOk++64Fri2mrs2LSgxFUVo+OtO7lOigt6IorQgVGCUkbZJq1looiqLEgwqMEqKxpBRFaU2owCghuqa3oiitCRUYJUTX9FYUpTWhAqMJUAVDUZTWgAqMEmJ0rreiKK0IFRglJGOSUhVDUZSWjwqMJkBNUoqitAZUYJQQ9apVFKU1oQKjCVANQ1GU1oAKjBKiE/cURWlNqMAoIW2qKwBIqIqhKEoroKhotUow/zx/f6a8u5w+nds0d1UURVGKRgVGCRnQvR2XHjmkuauhKIoSC2qSUhRFUUKhAkNRFEUJhQoMRVEUJRRFCwwRuV5EPhSR90TkURHp4th3hYjUisgCETnWkT7eSqsVkcmO9EEi8oaV/h8RqbbSa6zftdb+gcXWW1EURYlGHBrGc8BIY8yewEfAFQAiMhyYCIwAxgO3iEiFiFQANwPHAcOBs6y8AL8FbjLGDAbWARda6RcC66z0m6x8iqIoShNStMAwxjxrjGmwfs4E+lnbE4D7jTF1xpjFQC2wv/VXa4xZZIzZDtwPTJDUKkNHAg9Zx98FnOwo6y5r+yHgKNFViRRFUZqUuMcwLgCetrb7Aksd+5ZZaX7p3YH1DuFjp2eVZe3fYOVXFEVRmohQ8zBEZBrQ22PXlcaYx6w8VwINwL3xVS8aIjIJmAQwYMCA5qqGoihKqySUwDDGjAvaLyLnAycCR5lMAKXlQH9Htn5WGj7pXwBdRKTS0iKc+e2ylolIJdDZyu+u523AbVadVovIJ2Guz4MewJoCj22p6DXvGOg17xgUc827+O0oeqa3iIwHfgwcZozZ4tg1Bfi3iNwI7AwMAd4ktcT1EBEZREoQTATONsYYEZkBnEZqXOM84DFHWecBr1v7nzd5IvsZY3oWcU2zjDGjCz2+JaLXvGOg17xjUKprjiM0yF+AGuA5axx6pjHmImPMXBF5AJhHylR1iTGmEUBELgWmAhXAHcaYuVZZPwHuF5FfAXOA263024F7RKQWWEtKyCiKoihNSNECw3J19dt3LXCtR/pTwFMe6YtIeVG507cBpxdXU0VRFKUYdKa3N7c1dwWaAb3mHQO95h2Dklyz6CI/iqIoShhUw1AURVFCoQLDhV+cq5aOiPQXkRkiMk9E5orIZVZ6NxF5TkQWWv+7WukiIn+y7sN7IrJP815BYVjhaOaIyBPW71Yfr0xEuojIQ1aMt/kiMrY1P2cR+YH1Tn8gIveJSJvW+JxF5A4RWSUiHzjSIj9XETnPyr9QRM6LUgcVGA7yxLlq6TQAPzTGDAfGAJdY1zYZmG6MGQJMt35D6h4Msf4mAX9t+irHwmXAfMfvHSFe2R+BZ4wxw4BRpK6/VT5nEekLfA8YbYwZScrzciKt8zn/k1RcPieRnquIdAN+DhxAysHo57aQCYUxRv+sP2AsMNXx+wrgiuauV4mu9THgaGAB0MdK6wMssLZvBc5y5E/nayl/pCZ/TicVo+wJUnOA1gCV7udNys17rLVdaeWT5r6GAq65M7DYXffW+pzJhA3qZj23J4BjW+tzBgYCHxT6XIGzgFsd6Vn58v2phpGNX5yrVoWlhu8NvAH0MsassHZ9DvSytlvDvfgDqUmlSev3jhCvbBCwGrjTMsX9Q0Ta00qfszFmOfB74FNgBannNpvW/5xtoj7Xop63CowdDBHpADwMfN8Ys9G5z6S6HK3CbU5ETgRWGWNmN3ddmphKYB/gr8aYvYHNZMwUQKt7zl1JRbMeRCqiRHtyzTY7BE3xXFVgZBMU/6rFIyJVpITFvcaYR6zklSLSx9rfB1hlpbf0e3EQcJKILCEVauZIUrb9LlY8MvCOV0ZQvLIWwDJgmTHmDev3Q6QESGt9zuOAxcaY1caYeuARUs++tT9nm6jPtajnrQIjm7ew4lxZXhUTScWxavFIKm7L7cB8Y8yNjl12nC7Ijd/1DcvbYgywwaH6lj3GmCuMMf2MMQNJPcfnjTHnAHa8MvCOVwYh45WVI8aYz4GlIrK7lXQUqfA8rfI5kzJFjRGRdtY7bl9vq37ODqI+16nAMSLS1dLOjrHSwtHcgzjl9gccT2rlwI9JhW9v9jrFdF0Hk1JX3wPesf6OJ2W/nQ4sBKYB3az8Qspj7GPgfVJeKM1+HQVe++HAE9b2rqSCYNYCDwI1Vnob63ettX/X5q53Ede7FzDLetb/Bbq25ucM/BL4EPgAuIdUbLtW95yB+0iN09ST0iQvLOS5klq3qNb6+2aUOuhMb0VRFCUUapJSFEVRQqECQ1EURQmFCgxFURQlFCowFEVRlFCowFAURVFCoQJDURRFCYUKDEVRFCUUKjAURVGUUPw/1ADNNlmLwREAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}